name: Playwright Tests with Allure Report

on:
  push:
    branches:
      - master
      - develop
  pull_request:
    branches:
      - master
      - develop
  workflow_dispatch:
    inputs:
      test_files:
        description: "Specific test files to run (optional)"
        required: false
        default: ""
      environment:
        description: "Environment to test"
        required: false
        default: "dev"
        type: choice
        options:
          - dev
          - stage
          - prod
      user_role:
        description: "User role for authentication"
        required: false
        default: "b2c"
        type: choice
        options:
          - b2c
          - b2b
  schedule:
    - cron: "0 10,20 * * *"

env:
  NODE_VERSION: "20"
  JAVA_VERSION: "17"

jobs:
  test:
    name: Run Playwright Tests
    runs-on: ubuntu-latest
    timeout-minutes: 10

    permissions:
      contents: write

    outputs:
      test-status: ${{ steps.test-execution.outcome }}
      run-timestamp: ${{ steps.timestamp.outputs.timestamp }}
      run-start-time: ${{ steps.timing.outputs.start_time }}
      run-end-time: ${{ steps.timing.outputs.end_time }}
      run-duration: ${{ steps.timing.outputs.duration }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Generate timestamp and start timing
        id: timestamp
        run: echo "timestamp=$(date +'%Y%m%d_%H%M%S')" >> $GITHUB_OUTPUT

      - name: Record start time
        id: timing
        run: |
          START_TIME=$(date -u +"%Y-%m-%dT%H:%M:%S.%3NZ")
          echo "start_time=$START_TIME" >> $GITHUB_OUTPUT
          echo "START_TIME=$START_TIME" >> $GITHUB_ENV

      - name: Determine Environment and User Role
        id: set_env_vars
        run: |
          CURRENT_ENV=""
          CURRENT_USER_ROLE="${{ github.event.inputs.user_role || 'b2c' }}"

          if [ -n "${{ github.event.inputs.environment }}" ]; then
            CURRENT_ENV="${{ github.event.inputs.environment }}"
          elif [ "${{ github.ref }}" == "refs/heads/master" ]; then
            CURRENT_ENV="stage"
          elif [ "${{ github.ref }}" == "refs/heads/develop" ]; then
            CURRENT_ENV="stage"
          else
            CURRENT_ENV="stage"
          fi

          echo "Selected Environment: $CURRENT_ENV"
          echo "Selected User Role: $CURRENT_USER_ROLE"

          echo "ENV=$CURRENT_ENV" >> $GITHUB_ENV
          echo "USER_ROLE=$CURRENT_USER_ROLE" >> $GITHUB_ENV

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: "npm"

      - name: Setup Java & Allure
        uses: actions/setup-java@v4
        with:
          distribution: "temurin"
          java-version: ${{ env.JAVA_VERSION }}

      - name: Install global tools
        run: npm install -g allure-commandline@latest

      - name: Install dependencies
        run: npm ci

      - name: Get Playwright version
        id: playwright-version
        run: echo "version=$(npm list @playwright/test --depth=0 | grep @playwright/test | cut -d@ -f3)" >> $GITHUB_OUTPUT

      - name: Cache Playwright browsers
        uses: actions/cache@v4
        id: playwright-cache
        with:
          path: ~/.cache/ms-playwright
          key: playwright-browsers-${{ runner.os }}-${{ steps.playwright-version.outputs.version }}
          restore-keys: playwright-browsers-${{ runner.os }}-

      - name: Install Playwright browsers
        run: |
          if [ "${{ steps.playwright-cache.outputs.cache-hit }}" != "true" ]; then
            echo "🔄 Installing Chromium browser with dependencies..."
            npx playwright install chromium --with-deps
          else
            echo "🚀 Using cached browsers, installing system dependencies only..."
            npx playwright install-deps chromium
          fi

      - name: Run Playwright tests
        id: test-execution
        run: |
          TEST_FILES="${{ github.event.inputs.test_files }}"

          if [ -n "$TEST_FILES" ]; then
            echo "🎯 Running specific tests: $TEST_FILES"
            npx playwright test $TEST_FILES --reporter=dot,allure-playwright
          else
            echo "🔄 Running default test suite..."
            npx playwright test tests/smoke --reporter=dot,allure-playwright
          fi
        continue-on-error: true

      - name: Calculate duration and record end time
        id: calculate-duration
        if: always()
        run: |
          END_TIME=$(date -u +"%Y-%m-%dT%H:%M:%S.%3NZ")
          echo "end_time=$END_TIME" >> $GITHUB_OUTPUT

          START_EPOCH=$(date -d "$START_TIME" +%s)
          END_EPOCH=$(date -d "$END_TIME" +%s)
          DURATION=$((END_EPOCH - START_EPOCH))

          echo "duration=$DURATION" >> $GITHUB_OUTPUT
          echo "END_TIME=$END_TIME" >> $GITHUB_ENV
          echo "DURATION=$DURATION" >> $GITHUB_ENV

          echo "Test execution took: ${DURATION} seconds"

      - name: Extract Failed Tests Details
        if: always()
        run: |
          mkdir -p allure-results

          echo "🔍 Extracting failed test details..."

          # Criar arquivo JSON com detalhes dos testes falhos
          echo '[]' > allure-results/failed-tests-details.json

          if [ -d "allure-results" ]; then
            FAILED_TESTS_JSON='[]'
            
            # Processar arquivos de resultado do Allure
            for test_file in allure-results/*-result.json; do
              if [ -f "$test_file" ]; then
                echo "📄 Processing: $test_file"
                
                # Extrair informações do teste
                TEST_STATUS=$(jq -r '.status // "unknown"' "$test_file" 2>/dev/null || echo "unknown")
                
                if [ "$TEST_STATUS" = "failed" ] || [ "$TEST_STATUS" = "broken" ]; then
                  TEST_NAME=$(jq -r '.name // "N/A"' "$test_file" 2>/dev/null || echo "N/A")
                  TEST_FULL_NAME=$(jq -r '.fullName // "N/A"' "$test_file" 2>/dev/null || echo "N/A")
                  
                  # Extrair mensagem de erro de forma mais robusta
                  ERROR_MESSAGE=$(jq -r '.statusDetails.message // "N/A"' "$test_file" 2>/dev/null || echo "N/A")
                  
                  # Extrair stack trace se disponível
                  STACK_TRACE=$(jq -r '.statusDetails.trace // "N/A"' "$test_file" 2>/dev/null || echo "N/A")
                  
                  # Extrair labels para obter informações do arquivo
                  TEST_FILE_PATH=$(jq -r '.labels[] | select(.name == "suite") | .value' "$test_file" 2>/dev/null || echo "N/A")
                  
                  if [ "$TEST_FILE_PATH" = "N/A" ]; then
                    TEST_FILE_PATH=$(jq -r '.labels[] | select(.name == "parentSuite") | .value' "$test_file" 2>/dev/null || echo "N/A")
                  fi
                  
                  # Criar entrada JSON para o teste falho
                  FAILED_TEST_ENTRY=$(jq -n \
                    --arg name "$TEST_NAME" \
                    --arg fullName "$TEST_FULL_NAME" \
                    --arg status "$TEST_STATUS" \
                    --arg message "$ERROR_MESSAGE" \
                    --arg trace "$STACK_TRACE" \
                    --arg file "$TEST_FILE_PATH" \
                    '{
                      name: $name,
                      fullName: $fullName,
                      status: $status,
                      errorMessage: $message,
                      stackTrace: $trace,
                      testFile: $file
                    }')
                  
                  # Adicionar ao array de testes falhos
                  FAILED_TESTS_JSON=$(echo "$FAILED_TESTS_JSON" | jq --argjson entry "$FAILED_TEST_ENTRY" '. + [$entry]')
                  
                  echo "❌ Failed test found: $TEST_NAME in $TEST_FILE_PATH"
                fi
              fi
            done
            
            # Salvar detalhes dos testes falhos
            echo "$FAILED_TESTS_JSON" > allure-results/failed-tests-details.json
            
            # Debug: mostrar conteúdo
            echo "🔍 Failed tests details:"
            cat allure-results/failed-tests-details.json | jq .
            
            # Contar testes falhos
            FAILED_COUNT=$(echo "$FAILED_TESTS_JSON" | jq 'length')
            echo "📊 Total failed tests: $FAILED_COUNT"
          else
            echo "⚠️ No allure-results directory found"
          fi

      - name: Save Environment and Metadata to Allure Results
        if: always()
        run: |
          mkdir -p allure-results

          echo "🔧 DEBUG: Current environment variables:"
          echo "ENV: '$ENV'"
          echo "USER_ROLE: '$USER_ROLE'"
          echo "START_TIME: '$START_TIME'"
          echo "END_TIME: '$END_TIME'"
          echo "DURATION: '$DURATION'"

          # Validate and set defaults for environment variables
          FINAL_ENV="${ENV:-dev}"
          FINAL_USER_ROLE="${USER_ROLE:-b2c}"

          # Validate environment value
          if [[ ! "$FINAL_ENV" =~ ^(dev|stage|prod)$ ]]; then
            echo "⚠️ Invalid environment '$FINAL_ENV', defaulting to 'dev'"
            FINAL_ENV="dev"
          fi

          # Validate user role value
          if [[ ! "$FINAL_USER_ROLE" =~ ^(b2c|b2b)$ ]]; then
            echo "⚠️ Invalid user role '$FINAL_USER_ROLE', defaulting to 'b2c'"
            FINAL_USER_ROLE="b2c"
          fi

          echo "✅ Final values - ENV: '$FINAL_ENV', USER_ROLE: '$FINAL_USER_ROLE'"

          # Carregar detalhes dos testes falhos
          FAILED_TESTS_DETAILS='[]'
          if [ -f "allure-results/failed-tests-details.json" ]; then
            FAILED_TESTS_DETAILS=$(cat allure-results/failed-tests-details.json)
          fi

          # Save individual files for fallback compatibility
          echo "$FINAL_ENV" > allure-results/env.txt
          echo "$FINAL_USER_ROLE" > allure-results/user_role.txt

          echo "✅ Individual files saved"

          # Create consolidated metadata JSON
          cat > allure-results/test-execution-info.json << EOF
          {
            "environment": "$FINAL_ENV",
            "userRole": "$FINAL_USER_ROLE",
            "startTime": "$START_TIME",
            "endTime": "$END_TIME",
            "duration": $DURATION,
            "runNumber": "${{ github.run_number }}",
            "runId": "${{ github.run_id }}",
            "sha": "${{ github.sha }}",
            "ref": "${{ github.ref }}",
            "actor": "${{ github.actor }}",
            "timestamp": "${{ steps.timestamp.outputs.timestamp }}",
            "branch": "${{ github.ref_name }}",
            "eventName": "${{ github.event_name }}",
            "workflowName": "${{ github.workflow }}",
            "failedTestsDetails": $FAILED_TESTS_DETAILS
          }
          EOF

          # Create execution metadata for backward compatibility
          cat > allure-results/execution-metadata.json << EOF
          {
            "environment": "$FINAL_ENV",
            "userRole": "$FINAL_USER_ROLE",
            "startTime": "$START_TIME",
            "endTime": "$END_TIME",
            "duration": $DURATION,
            "runNumber": "${{ github.run_number }}",
            "runId": "${{ github.run_id }}",
            "sha": "${{ github.sha }}",
            "ref": "${{ github.ref }}",
            "actor": "${{ github.actor }}",
            "timestamp": "${{ steps.timestamp.outputs.timestamp }}",
            "failedTestsDetails": $FAILED_TESTS_DETAILS
          }
          EOF

          echo "✅ JSON metadata files created"

          # Verify files were created and show content
          echo "🔍 Verification - Files in allure-results:"
          ls -la allure-results/

          echo "🔍 Content verification:"
          echo "--- env.txt ---"
          cat allure-results/env.txt
          echo ""
          echo "--- user_role.txt ---"
          cat allure-results/user_role.txt
          echo ""
          echo "--- test-execution-info.json ---"
          cat allure-results/test-execution-info.json | jq .
          echo ""

      - name: Validate test results
        run: |
          if [ -d "allure-results" ] && [ "$(ls -A allure-results)" ]; then
            echo "✅ Allure results found ($(ls allure-results | wc -l) files)"
            
            # Verify our metadata files exist
            if [ -f "allure-results/test-execution-info.json" ]; then
              echo "✅ test-execution-info.json exists"
            else
              echo "❌ test-execution-info.json missing"
            fi
            
            if [ -f "allure-results/env.txt" ]; then
              echo "✅ env.txt exists"
            else
              echo "❌ env.txt missing"
            fi
            
            if [ -f "allure-results/user_role.txt" ]; then
              echo "✅ user_role.txt exists"
            else
              echo "❌ user_role.txt missing"
            fi
            
            if [ -f "allure-results/failed-tests-details.json" ]; then
              echo "✅ failed-tests-details.json exists"
              echo "📊 Failed tests count: $(cat allure-results/failed-tests-details.json | jq 'length')"
            else
              echo "❌ failed-tests-details.json missing"
            fi
          else
            echo "❌ No allure results found"
            exit 1
          fi
        if: always()

      - name: Generate Allure Report
        run: |
          allure generate allure-results --clean --output allure-report
          echo "📊 Allure report generated for run ${{ github.run_number }}"

          # Copy metadata files to the report directory for artifact upload
          if [ -f "allure-results/test-execution-info.json" ]; then
            cp allure-results/test-execution-info.json allure-report/
            echo "✅ Copied test-execution-info.json to report directory"
          fi

          if [ -f "allure-results/execution-metadata.json" ]; then
            cp allure-results/execution-metadata.json allure-report/
            echo "✅ Copied execution-metadata.json to report directory"
          fi

          if [ -f "allure-results/env.txt" ]; then
            cp allure-results/env.txt allure-report/
            echo "✅ Copied env.txt to report directory"
          fi

          if [ -f "allure-results/user_role.txt" ]; then
            cp allure-results/user_role.txt allure-report/
            echo "✅ Copied user_role.txt to report directory"
          fi

          if [ -f "allure-results/failed-tests-details.json" ]; then
            cp allure-results/failed-tests-details.json allure-report/
            echo "✅ Copied failed-tests-details.json to report directory"
          fi

          echo "🔍 Final allure-report directory contents:"
          ls -la allure-report/
        if: always()

      - name: Upload test artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: test-run-${{ steps.timestamp.outputs.timestamp }}
          path: |
            allure-report/
            allure-results/
            test-results/
          retention-days: 30

  create-index:
    name: Update Reports Index
    runs-on: ubuntu-latest
    needs: test
    if: always() && (github.ref == 'refs/heads/master' || github.event_name == 'workflow_dispatch')

    permissions:
      contents: write

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          path: repo
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Checkout gh-pages
        uses: actions/checkout@v4
        with:
          ref: gh-pages
          path: gh-pages
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Install jq
        run: sudo apt-get update && sudo apt-get install -y jq

      - name: Download test report artifact
        uses: actions/download-artifact@v4
        with:
          name: test-run-${{ needs.test.outputs.run-timestamp }}
          path: downloaded-report

      - name: DEBUG - Check downloaded artifact
        run: |
          echo "🔍 ===== ARTIFACT ANALYSIS ====="
          echo "📁 Structure of downloaded-report:"
          find downloaded-report -type f -name "*.json" -o -name "*.txt" | head -20

          echo ""
          echo "🔍 Checking for metadata files in multiple locations:"

          # Check in allure-report directory (where we copied them)
          for file in test-execution-info.json execution-metadata.json env.txt user_role.txt failed-tests-details.json; do
            if [ -f "downloaded-report/allure-report/$file" ]; then
              echo "✅ Found $file in allure-report/"
            else
              echo "❌ Missing $file in allure-report/"
            fi
          done

          # Check in allure-results directory (original location)
          for file in test-execution-info.json execution-metadata.json env.txt user_role.txt failed-tests-details.json; do
            if [ -f "downloaded-report/allure-results/$file" ]; then
              echo "✅ Found $file in allure-results/"
            else
              echo "❌ Missing $file in allure-results/"
            fi
          done

          echo "🔍 ===== END ANALYSIS ====="

      - name: Move downloaded report to gh-pages structure
        run: |
          REPORT_DIR="gh-pages/reports/${{ needs.test.outputs.run-timestamp }}"
          mkdir -p "$REPORT_DIR"

          # Copy the Allure report content
          if [ -d "downloaded-report/allure-report" ]; then
              cp -R downloaded-report/allure-report/. "$REPORT_DIR"
              echo "✅ Copied allure-report content to $REPORT_DIR"
          else
              echo "❌ Error: allure-report directory not found"
              exit 1
          fi

          # Ensure metadata files are present in the report directory
          # Try multiple source locations
          for source_dir in "downloaded-report/allure-report" "downloaded-report/allure-results"; do
            if [ -d "$source_dir" ]; then
              echo "🔍 Checking source directory: $source_dir"
              
              # Copy metadata files if they exist in this source
              for file in test-execution-info.json execution-metadata.json env.txt user_role.txt failed-tests-details.json; do
                if [ -f "$source_dir/$file" ]; then
                  cp "$source_dir/$file" "$REPORT_DIR/$file"
                  echo "✅ Copied $file from $source_dir"
                fi
              done
            fi
          done

          echo "🔍 Final report directory contents:"
          ls -la "$REPORT_DIR"

          # Verify metadata files are present
          echo "🔍 Metadata files verification:"
          for file in test-execution-info.json execution-metadata.json env.txt user_role.txt failed-tests-details.json; do
            if [ -f "$REPORT_DIR/$file" ]; then
              echo "✅ $file present in report directory"
            else
              echo "❌ $file missing in report directory"
            fi
          done

      - name: Generate reports data
        run: |
          set -e
          cd gh-pages || { echo "❌ Could not access gh-pages directory"; exit 1; }

          echo '{"reports": [' > reports.json
          REPORTS_COUNT=0

          if [ -d "reports" ] && [ "$(ls -A reports)" ]; then
            for dir in $(ls -1t reports/); do
              if [ -d "reports/$dir" ] && [ -f "reports/$dir/widgets/summary.json" ]; then
                echo "🔍 Processing report: $dir"

                if [ "$REPORTS_COUNT" -gt 0 ]; then
                  echo -n "," >> reports.json
                fi

                timestamp_dir=$(echo "$dir" | sed 's/_/ /')
                date_part=$(echo "$dir" | cut -d_ -f1)
                time_part=$(echo "$dir" | cut -d_ -f2)
                year=${date_part:0:4}
                month=${date_part:4:2}
                day=${date_part:6:2}
                hour=${time_part:0:2}
                minute=${time_part:2:2}
                second=${time_part:4:2}
                date_formatted="${year}-${month}-${day}T${hour}:${minute}:${second}Z"

                summary_data=$(cat "reports/$dir/widgets/summary.json" 2>/dev/null || echo '{}')
                report_status="unknown"
                test_summary_json="null"
                failed_tests_details="[]"

                if [ -n "$summary_data" ] && [ "$summary_data" != "{}" ]; then
                  test_summary_json=$(echo "$summary_data" | jq -c . 2>/dev/null || echo "null")
                  
                  passed=$(echo "$summary_data" | jq -r '.statistic.passed // 0' 2>/dev/null || echo "0")
                  failed=$(echo "$summary_data" | jq -r '.statistic.failed // 0' 2>/dev/null || echo "0")
                  broken=$(echo "$summary_data" | jq -r '.statistic.broken // 0' 2>/dev/null || echo "0")

                  if [ "$failed" -gt 0 ] || [ "$broken" -gt 0 ]; then
                    report_status="failure"
                  elif [ "$passed" -gt 0 ]; then
                    report_status="success"
                  fi
                fi

                # Load failed tests details from the correct locations
                if [ -f "reports/$dir/failed-tests-details.json" ]; then
                  failed_tests_details=$(cat "reports/$dir/failed-tests-details.json" 2>/dev/null || echo "[]")
                  echo "✅ Loaded failed tests details from failed-tests-details.json"
                elif [ -f "reports/$dir/test-execution-info.json" ]; then
                  failed_tests_details=$(cat "reports/$dir/test-execution-info.json" | jq -r '.failedTestsDetails // "[]"' 2>/dev/null || echo "[]")
                  echo "✅ Loaded failed tests details from test-execution-info.json"
                elif [ -f "reports/$dir/execution-metadata.json" ]; then
                  failed_tests_details=$(cat "reports/$dir/execution-metadata.json" | jq -r '.failedTestsDetails // "[]"' 2>/dev/null || echo "[]")
                  echo "✅ Loaded failed tests details from execution-metadata.json"
                fi

                # Ensure failed_tests_details is valid JSON
                if ! echo "$failed_tests_details" | jq . >/dev/null 2>&1; then
                  echo "⚠️ Invalid JSON in failed tests details, resetting to empty array"
                  failed_tests_details="[]"
                fi

                CURRENT_ENV="unknown"
                CURRENT_USER_ROLE="unknown"
                execution_metadata="{}"

                if [ -f "reports/$dir/test-execution-info.json" ]; then
                  consolidated_data=$(cat "reports/$dir/test-execution-info.json" 2>/dev/null || echo '{}')
                  if [ "$consolidated_data" != "{}" ] && [ -n "$consolidated_data" ]; then
                    CURRENT_ENV=$(echo "$consolidated_data" | jq -r '.environment // "unknown"' 2>/dev/null || echo "unknown")
                    CURRENT_USER_ROLE=$(echo "$consolidated_data" | jq -r '.userRole // "unknown"' 2>/dev/null || echo "unknown")
                    execution_metadata="$consolidated_data"
                  fi
                fi

                if [ "$CURRENT_ENV" = "unknown" ] && [ -f "reports/$dir/execution-metadata.json" ]; then
                  execution_metadata_content=$(cat "reports/$dir/execution-metadata.json" 2>/dev/null || echo '{}')
                  if [ "$execution_metadata_content" != "{}" ] && [ -n "$execution_metadata_content" ]; then
                    CURRENT_ENV=$(echo "$execution_metadata_content" | jq -r '.environment // "unknown"' 2>/dev/null || echo "unknown")
                    CURRENT_USER_ROLE=$(echo "$execution_metadata_content" | jq -r '.userRole // "unknown"' 2>/dev/null || echo "unknown")
                    execution_metadata="$execution_metadata_content"
                  fi
                fi

                if [ "$CURRENT_ENV" = "unknown" ] && [ -f "reports/$dir/env.txt" ]; then
                  CURRENT_ENV=$(cat "reports/$dir/env.txt" 2>/dev/null | tr -d '\n\r\t ' || echo "unknown")
                fi

                if [ "$CURRENT_USER_ROLE" = "unknown" ] && [ -f "reports/$dir/user_role.txt" ]; then
                  CURRENT_USER_ROLE=$(cat "reports/$dir/user_role.txt" 2>/dev/null | tr -d '\n\r\t ' || echo "unknown")
                fi

                RUN_NUMBER=$(echo "$execution_metadata" | jq -r '.runNumber // "N/A"' 2>/dev/null || echo "N/A")
                RUN_ID=$(echo "$execution_metadata" | jq -r '.runId // "N/A"' 2>/dev/null || echo "N/A")
                START_TIME_META=$(echo "$execution_metadata" | jq -r '.startTime // "N/A"' 2>/dev/null || echo "N/A")
                END_TIME_META=$(echo "$execution_metadata" | jq -r '.endTime // "N/A"' 2>/dev/null || echo "N/A")
                DURATION_META=$(echo "$execution_metadata" | jq -r '.duration // "N/A"' 2>/dev/null || echo "N/A")

                # Create JSON entry with proper failed tests details
                JSON_ENTRY=$(jq -n \
                  --arg id "$dir" \
                  --arg timestamp "$timestamp_dir" \
                  --arg dateFormatted "$date_formatted" \
                  --arg status "$report_status" \
                  --arg runNumber "$RUN_NUMBER" \
                  --arg runId "$RUN_ID" \
                  --arg path "reports/$dir/" \
                  --argjson testSummary "$test_summary_json" \
                  --arg environment "$CURRENT_ENV" \
                  --arg userRole "$CURRENT_USER_ROLE" \
                  --argjson executionMetadata "$execution_metadata" \
                  --arg startTime "$START_TIME_META" \
                  --arg endTime "$END_TIME_META" \
                  --arg duration "$DURATION_META" \
                  --argjson failedTestsDetails "$failed_tests_details" \
                  '{
                    id: $id,
                    timestamp: $timestamp,
                    dateFormatted: $dateFormatted,
                    status: $status,
                    runNumber: $runNumber,
                    runId: $runId,
                    path: $path,
                    testSummary: $testSummary,
                    environment: $environment,
                    userRole: $userRole,
                    executionMetadata: $executionMetadata,
                    startTime: $startTime,
                    endTime: $endTime,
                    duration: $duration,
                    failedTestsDetails: $failedTestsDetails
                  }')
                
                echo "$JSON_ENTRY" >> reports.json
                REPORTS_COUNT=$((REPORTS_COUNT + 1))
                
                # Debug: show failed tests count
                FAILED_COUNT=$(echo "$failed_tests_details" | jq 'length' 2>/dev/null || echo "0")
                echo "📊 Report $dir has $FAILED_COUNT failed tests"
              fi
            done
          fi

          echo ']}' >> reports.json

          if ! jq . reports.json > reports.json.tmp; then
            echo "❌ Malformed JSON in reports.json"
            cat reports.json
            exit 1
          fi
          mv reports.json.tmp reports.json

          echo "✅ reports.json generated successfully with $REPORTS_COUNT reports"

      - name: Create dynamic index
        run: |
          cp repo/templates/dashboard-template.html gh-pages/index.html

      - name: Commit index
        run: |
          cd gh-pages
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add index.html reports.json reports/
          git commit -m "Update reports index and data for run #${{ github.run_number }}" || exit 0
          git push

  cleanup:
    name: Cleanup Old Reports
    runs-on: ubuntu-latest
    # Este job só precisa ser executado se o job 'create-index' for bem-sucedido ou falhar,
    # e apenas no branch 'master' para evitar limpezas desnecessárias em branches de desenvolvimento.
    needs: [test, create-index]
    if: always() && github.ref == 'refs/heads/master'

    steps:
      - name: Delete old test run artifacts
        uses: actions/github-script@v7
        with:
          script: |
            const { owner, repo } = context.repo;

            // Lista todos os artefatos do repositório. O per_page pode ser ajustado se você tiver muitos artefatos.
            const artifacts = await github.rest.actions.listArtifactsForRepo({
              owner,
              repo,
              per_page: 100 // Aumente se você tiver mais de 100 artefatos frequentemente
            });

            // Filtra os artefatos que são relatórios de teste (começam com 'test-run-'),
            // ordena do mais recente para o mais antigo e pega os que estão além dos 50 mais recentes.
            const testArtifactsToDelete = artifacts.data.artifacts
              .filter(a => a.name.startsWith('test-run-')) // Filtra apenas seus artefatos de relatório
              .sort((a, b) => new Date(b.created_at) - new Date(a.created_at)) // Ordena por data de criação (mais recente primeiro)
              .slice(2); // Pega todos os artefatos a partir do 51º (ou seja, os mais antigos que os 50 mais recentes)

            if (testArtifactsToDelete.length === 0) {
              console.log('Nenhum artefato de relatório antigo para deletar.');
              return;
            }

            console.log(`Deletando ${testArtifactsToDelete.length} artefatos de relatório antigos...`);
            for (const artifact of testArtifactsToDelete) {
              try {
                await github.rest.actions.deleteArtifact({
                  owner,
                  repo,
                  artifact_id: artifact.id
                });
                console.log(`Artefato deletado: ${artifact.name} (ID: ${artifact.id}, Criado em: ${artifact.created_at})`);
              } catch (error) {
                console.error(`Falha ao deletar artefato ${artifact.name} (ID: ${artifact.id}): ${error.message}`);
              }
            }